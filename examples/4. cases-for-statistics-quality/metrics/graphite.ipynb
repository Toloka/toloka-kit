{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and show metrics in Graphite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will learn how to collect metrics using Toloka-kit and\n",
    "send them to remote metrics server (we will use [Graphite](https://graphiteapp.org) but switching to any other solution is very easy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install toloka-kit==0.1.26\n",
    "!pip install crowd-kit==1.0.0\n",
    "\n",
    "import socket\n",
    "import asyncio\n",
    "import logging\n",
    "import getpass\n",
    "\n",
    "import toloka.metrics as metrics\n",
    "import toloka.client as toloka\n",
    "from toloka.metrics import MetricCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toloka_client = toloka.TolokaClient(getpass.getpass('Enter your OAuth token: '), 'PRODUCTION') # Or switch to 'SANDBOX'\n",
    "print(toloka_client.get_requester())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will run pipeline from [Streaming pipeline example](https://github.com/Toloka/toloka-kit/tree/main/examples/6.streaming_pipelines/streaming_pipelines.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Toloka/toloka-kit/blob/main/examples/6.streaming_pipelines/streaming_pipelines.ipynb).\n",
    "If you are running this jupyter notebook in colab please download necessary script with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet --show-progress \"https://raw.githubusercontent.com/Toloka/toloka-kit/main/examples/metrics/find_items_pipeline.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from find_items_pipeline import FindItemsPipeline\n",
    "pipeline = FindItemsPipeline(client=toloka_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create projects and pools needed for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.init_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring metrics collection in Graphite\n",
    "\n",
    "You need to [configure](https://graphite.readthedocs.io/en/stable/install.html) Graphite server before proceeding\n",
    "to this section. An easy option might be using official docker container. Selection of user interface is up to you\n",
    "(during creation of this example we used [Grafana](https://grafana.com))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your Graphite instance url and port\n",
    "CARBON_ADDRESS = 'localhost'\n",
    "CARBON_PORT = 2003\n",
    "\n",
    "try:\n",
    "    sock = socket.socket()\n",
    "    sock.connect((CARBON_ADDRESS, CARBON_PORT))\n",
    "    sock.close()\n",
    "except ConnectionRefusedError:\n",
    "    raise RuntimeError('Graphite server is unreachable!')\n",
    "else:\n",
    "    print('Congratulations, connected to Graphite server!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a callback for handling metrics values. We'll use it to store the data on a Graphite server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphiteLogger:\n",
    "    def __init__(self, carbon_address, carbon_port, use_ipv6=False):\n",
    "        self.carbon_address = carbon_address\n",
    "        self.carbon_port = carbon_port\n",
    "        self.use_ipv6 = use_ipv6\n",
    "        self.logger = logging.getLogger('GraphiteLogger')\n",
    "\n",
    "    def __call__(self, metric_dict):\n",
    "        if self.use_ipv6:\n",
    "            s = socket.socket(socket.AF_INET6)\n",
    "            s.connect((self.carbon_address, self.carbon_port, 0, 0))\n",
    "        else:\n",
    "            s = socket.socket()\n",
    "            s.connect((self.carbon_address, self.carbon_port))\n",
    "\n",
    "        for metric in metric_dict:\n",
    "            for timestamp, value in metric_dict[metric]:\n",
    "                s.sendall(\n",
    "                    f'{metric} {value} {timestamp.timestamp()}\\n'.encode()\n",
    "                )\n",
    "                self.logger.log(\n",
    "                    logging.INFO,\n",
    "                    f'Logged {metric} {value} {timestamp.timestamp()}'\n",
    "                )\n",
    "        s.close()\n",
    "\n",
    "\n",
    "graphite_logger = GraphiteLogger(\n",
    "    CARBON_ADDRESS, CARBON_PORT,\n",
    "    # specify use_ipv6=True if your Graphite server is available only via IPv6\n",
    "    # (this may be the case if you are running Graphite inside docker hosted in MacOS)\n",
    "    use_ipv6=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sending metrics to Graphite we have to:\n",
    "- Define which metrics we'll collect.\n",
    "- Describe what we'll do with these metrics, as a callable functor.\n",
    "- Define a TolokaClient for each metric.\n",
    "- Asynchronously call `run` for the MetricCollector instance.\n",
    "\n",
    "For this example we will collect a number of submitted assignments, accepted assignments and total expenses for each pool. All available metrics can be found in the [documentation](https://toloka.ai/en/docs/toloka-kit/reference/toloka.metrics.metrics.BaseMetric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_collector = MetricCollector(\n",
    "    [\n",
    "        # Assignments in pools. We will track submitted assignments and\n",
    "        # accepted assignments counts for every pool.\n",
    "        metrics.AssignmentsInPool(\n",
    "            pipeline.verification_pool.id,\n",
    "            submitted_name='verification_pool.submitted_assignments',\n",
    "            accepted_name='verification_pool.accepted_assignments',\n",
    "        ),\n",
    "        metrics.AssignmentsInPool(\n",
    "            pipeline.find_items_pool.id,\n",
    "            submitted_name='find_items_pool.submitted_assignments',\n",
    "            accepted_name='find_items_pool.accepted_assignments',\n",
    "        ),\n",
    "        metrics.AssignmentsInPool(\n",
    "            pipeline.sbs_pool.id,\n",
    "            submitted_name='sbs_pool.submitted_assignments',\n",
    "            accepted_name='sbs_pool.accepted_assignments',\n",
    "        ),\n",
    "        # Budget spent for every pool\n",
    "        metrics.SpentBudgetOnPool(\n",
    "            pipeline.verification_pool.id,\n",
    "            'verification_pool.expenses'\n",
    "        ),\n",
    "        metrics.SpentBudgetOnPool(\n",
    "            pipeline.find_items_pool.id,\n",
    "            'find_items_pool.expenses'\n",
    "        ),\n",
    "        metrics.SpentBudgetOnPool(\n",
    "            pipeline.sbs_pool.id,\n",
    "            'sbs_pool.expenses'\n",
    "        )\n",
    "    ],\n",
    "    callback=graphite_logger\n",
    ")\n",
    "\n",
    "# You can specify toloka_client argument in each metric instead of calling\n",
    "# bind_client if you want to use different clients for different metrics\n",
    "metrics.bind_client(metric_collector.metrics, toloka_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running pipeline\n",
    "\n",
    "Let's try to launch our pipeline and see metrics updated. Metrics will be sent to configured Graphite server.\n",
    "\n",
    "⚠️ **Be careful**:\n",
    "real projects will be created and money will be spent in case of running in production environment! ⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab is using a global event pool,\n",
    "# so in order to run our pipeline we have to apply nest_asyncio to create an inner pool\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    import nest_asyncio, asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.get_event_loop().run_until_complete(asyncio.gather(metric_collector.run(), pipeline.run()))\n",
    "else:\n",
    "    await asyncio.gather(metric_collector.run(), pipeline.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of metrics displayed in Grafana with Graphite as the Datasource after pipeline completion.\n",
    "\n",
    "<table  align=\"center\">\n",
    "  <tr><td>\n",
    "    <img src=\"./img/grafana_metrics.png\" width=\"1000\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 2.</b> Grafana web view.\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Graphite in production\n",
    "In normal usage it's better to gather metrics from Toloka once in ten minutes or less often. So you must prepare your graphite for that. Typically it already has `count` type of aggregation, that looks like that:\n",
    "```\n",
    "    [count]\n",
    "    pattern = \\.count$\n",
    "    xFilesFactor = 0\n",
    "    aggregationMethod = sum\n",
    "```\n",
    "\n",
    "\n",
    "It means, that all new metrics that end on ```.count``` will be processed like that: sum all of their values when graphite needs to aggregate this metric on some interval.\n",
    "\n",
    "\n",
    "But for metrics that cannot be summed, for example, completion percentage, by default it's no useful type. So you need to add them to the ```storage-aggregation.conf```:\n",
    "```\n",
    "    [metric]\n",
    "    pattern=_metric$\n",
    "    xFileFactor = 0\n",
    "    aggregationMethod = average\n",
    "```\n",
    "\n",
    "It means if you send a metric that ends on ```_metric``` to graphite, it will aggregate this metric like an average on any interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you need to set up right retention for this metric in ```storage-schemas.conf```, for example:\n",
    "```\n",
    "    [metric]\n",
    "    pattern = _metric$\n",
    "    retentions = 10m:7d,1h:360d\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
